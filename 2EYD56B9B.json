{"paragraph_1578044987529_1240899810":{"orderNumber":2,"id":"paragraph_1578044987529_1240899810","text":"%flink.ssql\n\nDROP TABLE IF EXISTS source_kafka;\n\nCREATE TABLE source_kafka (\n    status  STRING,\n    direction STRING,\n    event_ts BIGINT\n) WITH (\n  'connector.type' = 'kafka',       \n  'connector.version' = 'universal',\n  'connector.topic' = 'generated.events',\n  'connector.startup-mode' = 'earliest-offset',\n  'connector.properties.zookeeper.connect' = 'localhost:2181',\n  'connector.properties.bootstrap.servers' = 'localhost:9092',\n  'connector.properties.group.id' = 'testGroup',\n  'connector.startup-mode' = 'earliest-offset',\n  'format.type'='json',\n  'update-mode' = 'append'\n);","title":"Create kafka source table"},"paragraph_1587959422055_1513725291":{"orderNumber":1,"id":"paragraph_1587959422055_1513725291","text":"%flink.conf\n\n# You need to run this paragraph first before running any flink code.\n\nflink.execution.packages\torg.apache.flink:flink-connector-kafka_2.11:1.10.0,org.apache.flink:flink-connector-kafka-base_2.11:1.10.0,org.apache.flink:flink-json:1.10.0","title":"Configure flink kafka connector"},"paragraph_1578905715189_33634195":{"orderNumber":4,"id":"paragraph_1578905715189_33634195","text":"%flink.ssql\n\ninsert into sink_kafka select status, direction, cast(event_ts/1000000000 as timestamp(3)) from source_kafka where status <> 'foo'\n","title":"Transform the data in source table and write it to sink table"},"paragraph_1579058345516_-1005807622":{"orderNumber":5,"id":"paragraph_1579058345516_-1005807622","text":"%flink.ssql(type=update)\n\nselect * from sink_kafka order by event_ts desc limit 10;","title":"Preview sink table result"},"paragraph_1579054287919_-61477360":{"orderNumber":0,"id":"paragraph_1579054287919_-61477360","text":"%md\n\nThis tutorial demonstrate how to use Flink do streaming processing via its streaming sql + udf. In this tutorial, we read data from kafka queue and do some simple processing (just filtering here) and then write it back to another kafka queue. We use this [docker](https://zeppelin-kafka-connect-datagen.readthedocs.io/en/latest/) to create kafka cluster and source data \n\n* Make sure you add the following ip host name mapping to your hosts file, otherwise you may not be able to connect to the kafka cluster in docker\n\n```\n127.0.0.1   broker\n```\n\nUse the following command to generate the sample data.\n\n```\ncurl -X POST http://localhost:8083/connectors \\\n-H 'Content-Type:application/json' \\\n-H 'Accept:application/json' \\\n-d @connect.source.datagen.json\n```","title":"Overview"},"paragraph_1579058056677_-1981512536":{"orderNumber":6,"id":"paragraph_1579058056677_-1981512536","text":"%flink.ssql\n","title":null},"paragraph_1578905686087_1273839451":{"orderNumber":3,"id":"paragraph_1578905686087_1273839451","text":"%flink.ssql\n\nDROP TABLE IF EXISTS sink_kafka;\n\nCREATE TABLE sink_kafka (\n    status  STRING,\n    direction STRING,\n    event_ts TIMESTAMP(3),\n    WATERMARK FOR event_ts AS event_ts - INTERVAL '5' SECOND\n) WITH (\n  'connector.type' = 'kafka',       \n  'connector.version' = 'universal',    \n  'connector.topic' = 'generated.events2',\n  'connector.properties.zookeeper.connect' = 'localhost:2181',\n  'connector.properties.bootstrap.servers' = 'localhost:9092',\n  'connector.properties.group.id' = 'testGroup',\n  'format.type'='json',\n  'update-mode' = 'append'\n)\n\n","title":"Create kafka sink table"}}