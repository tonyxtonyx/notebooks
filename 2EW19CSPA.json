"{\"paragraph_1578669828368_-1923137601\":{\"id\":\"paragraph_1578669828368_-1923137601\",\"text\":\"%flink.bsql\\n\\ninsert into bank select T.* from bank_raw, LATERAL TABLE(parse(content)) as T(age, job,  marital, education, `default`, balance, housing, loan, contact, `day`, `month`, duration, campaign, pdays,  previous,  poutcome, y) \",\"title\":\"Parse the data and write it into sink table\"},\"paragraph_1578888628353_1621411444\":{\"id\":\"paragraph_1578888628353_1621411444\",\"text\":\"%flink\\n\\nimport org.apache.flink.api.java.typeutils.RowTypeInfo\\nimport org.apache.flink.api.common.typeinfo.Types\\nimport org.apache.flink.api.java.typeutils._\\nimport org.apache.flink.api.scala.typeutils._\\nimport org.apache.flink.api.scala._\\n\\nclass Person(val age:Int, val job: String, val marital: String, val education: String, val default: String, val balance: String, val housing: String, val loan: String, val contact: String, val day: String, val month: String, val duration: Int, val campaign: Int, val pdays: Int, val previous: Int, val poutcome: String, val y: String)\\n\\nclass ParseFunction extends TableFunction[Row] {\\n  def eval(line: String) {\\n    val tokens = line.split(\\\";\\\")\\n    \\/\\/ parse the line\\n    if (!line.startsWith(\\\"\\\\\\\"age\\\\\\\"\\\")) {\\n      collect(Row.of(new Integer(tokens(0).toInt), normalize(tokens(1)), normalize(tokens(2)), normalize(tokens(3)), normalize(tokens(4)), normalize(tokens(5)), normalize(tokens(6)), normalize(tokens(7)), normalize(tokens(8)), normalize(tokens(9)), normalize(tokens(10)), new Integer(tokens(11).toInt),  new Integer(tokens(12).toInt),  \\n           new Integer(tokens(13).toInt), new Integer(tokens(14).toInt),  normalize(tokens(15)), normalize(tokens(16))))\\n    }\\n  }\\n  \\n  override def getResultType() = {\\n    val cls = classOf[Person]\\n    Types.ROW(Types.INT, Types.STRING, Types.STRING, Types.STRING,Types.STRING,Types.STRING,Types.STRING,Types.STRING,Types.STRING,Types.STRING,Types.STRING,\\n    Types.INT,  Types.INT, Types.INT, Types.INT, Types.STRING, Types.STRING)\\n  }\\n\\n  \\/\\/ remove the quote\\n  private def normalize(token: String) = {\\n      if (token.startsWith(\\\"\\\\\\\"\\\")) {\\n          token.substring(1, token.length - 1)\\n      } else {\\n          token\\n      }\\n  }\\n}\\n\\nbtenv.registerFunction(\\\"parse\\\", new ParseFunction())\",\"title\":\"Define UDTF ParseFunction to parse the raw data\"},\"paragraph_1578044954921_-1188487356\":{\"id\":\"paragraph_1578044954921_-1188487356\",\"text\":\"%flink.bsql\\n\\nDROP TABLE IF EXISTS bank_raw;\\nCREATE TABLE bank_raw (\\n   content STRING\\n) WITH (\\n'format.field-delimiter'='\\\\n',\\n'connector.type'='filesystem',\\n'format.derive-schema'='true',\\n'connector.path'='\\/tmp\\/bank.csv',\\n'format.type'='csv'\\n);\",\"title\":\"Define source table which represents the raw data\"},\"paragraph_1578068480238_-1678045273\":{\"id\":\"paragraph_1578068480238_-1678045273\",\"text\":\"%flink.bsql\\n\\nselect * from bank limit 10\\n\",\"title\":\"Preview output data\"},\"paragraph_1587958743728_1444404682\":{\"id\":\"paragraph_1587958743728_1444404682\",\"text\":\"%flink.bsql\\n\\nshow tables\",\"title\":\"Show tables\"},\"paragraph_1579061037737_-1577558456\":{\"id\":\"paragraph_1579061037737_-1577558456\",\"text\":\"%flink\\n\\nval table = btenv.sqlQuery(\\\"select * from bank limit 10\\\")\\nz.show(table)\",\"title\":\"Display table via z.show\"},\"paragraph_1588693027989_1331448600\":{\"id\":\"paragraph_1588693027989_1331448600\",\"text\":\"%flink.pyflink\\n\",\"title\":null},\"paragraph_1588690392097_1159956807\":{\"id\":\"paragraph_1588690392097_1159956807\",\"text\":\"%flink.pyflink\\n\\ntable = bt_env.sql_query(\\\"select * from bank limit 10\\\")\\nz.show(table)\",\"title\":\"Display table via z.show in PyFlink\"},\"paragraph_1579053112778_2010129053\":{\"id\":\"paragraph_1579053112778_2010129053\",\"text\":\"%sh\\n\\nhead -n 10 \\/tmp\\/bank.csv\",\"title\":\"Raw Data Preview\"},\"paragraph_1579052523153_721650872\":{\"id\":\"paragraph_1579052523153_721650872\",\"text\":\"%md\\n\\nThis tutorial demonstrates how to use Flink do batch ETL via its batch sql + udf (scala, python & hive). Here's what we do in this tutorial\\n\\n* Download [bank](https:\\/\\/archive.ics.uci.edu\\/ml\\/datasets\\/bank+marketing) data via shell interpreter to local\\n* Process the raw data via flink batch sql & scala udf which parse and clean the raw data\\n* Write the structured and cleaned data to another flink table via sql\\n\",\"title\":\"Overview\"},\"paragraph_1578045094400_1030344935\":{\"id\":\"paragraph_1578045094400_1030344935\",\"text\":\"%sh\\n\\ncd \\/tmp\\nrm -rf bank*\\nwget https:\\/\\/archive.ics.uci.edu\\/ml\\/machine-learning-databases\\/00222\\/bank.zip\\nunzip bank.zip\\n# upload data to hdfs if you want to run it in yarn mode\\n# hadoop fs -put \\/tmp\\/bank.csv \\/tmp\\/bank.csv\\n\",\"title\":\"Download bank data\"},\"paragraph_1578045204379_-1427374232\":{\"id\":\"paragraph_1578045204379_-1427374232\",\"text\":\"%flink.bsql\\n\\nDROP TABLE IF EXISTS bank;\\nCREATE TABLE bank (\\n    age int, \\n    job string,\\n    marital string,\\n    education string,\\n    `default` string,\\n    balance string,\\n    housing string,\\n    loan string,\\n    contact string, \\n    `day` string,\\n    `month` string,\\n    duration int,\\n    campaign int,\\n    pdays int,\\n    previous int,\\n    poutcome string,\\n    y string\\n) WITH (\\n'format.field-delimiter'=',',\\n'connector.type'='filesystem',\\n'format.derive-schema'='true',\\n'connector.path'='\\/tmp\\/bank_cleaned',\\n'format.type'='csv'\\n);\",\"title\":\"Define sink table which represents the cleaned data\"},\"paragraph_1579061020460_-113987164\":{\"id\":\"paragraph_1579061020460_-113987164\",\"text\":\"%sh\\n\\nrm -rf \\/tmp\\/bank_cleaned\\n#hadoop fs -rmr \\/tmp\\/bank_cleaned\",\"title\":\"Clean data\"}}"