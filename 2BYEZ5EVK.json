"{\"20160927-161850_1560940440\":{\"id\":\"20160927-161850_1560940440\",\"text\":\"%md\\n\\nAfter the interpreters are created you will need to 'bind' them by clicking on the little gear in the top right corner, scrolling to the top, and clicking on `mahoutFlink` and `mahoutSpark` so that they are highlighted in blue.\\n\\n#### Running Mahout code\\n\\nYou will need to import certain libraries, and declare the _Mahout Distributed Context_ when you first start your notebook using the interpreters. \\n\\nIf using Apache Flink the code you need to run is:\\n```scala\\n%flinkMahout\\n\\nimport org.apache.flink.api.scala._\\nimport org.apache.mahout.math.drm._\\nimport org.apache.mahout.math.drm.RLikeDrmOps._\\nimport org.apache.mahout.flinkbindings._\\nimport org.apache.mahout.math._\\nimport scalabindings._\\nimport RLikeOps._\\n\\n\\nimplicit val ctx = new FlinkDistributedContext(benv)\\n```\\n\\nIf using Apache Spark the code you need to run is\\n```scala\\n%sparkMahout\\n\\nimport org.apache.mahout.math._\\nimport org.apache.mahout.math.scalabindings._\\nimport org.apache.mahout.math.drm._\\nimport org.apache.mahout.math.scalabindings.RLikeOps._\\nimport org.apache.mahout.math.drm.RLikeDrmOps._\\nimport org.apache.mahout.sparkbindings._\\n\\nimplicit val sdc: org.apache.mahout.sparkbindings.SparkDistributedContext = sc2sdc(sc)\\n```\\n\\n__Note: For Apache Mahout on Apache Spark you must be running Spark 1.5.x or 1.6.x.  We are working hard on supporting Spark 2.0__\\nIn the meantime, feel free to play with Mahout on Flink and then simple _copy and paste your Mahout code to Spark once it is supported!_\\n\\n### A Side by Side Example\\n\",\"title\":null},\"20160927-165312_1668894932\":{\"id\":\"20160927-165312_1668894932\",\"text\":\"%md\\n\\n### Taking advantage of Zeppelin Resource Pools\\n\\nOne of the major motivations for integrating Apache Mahout with Apache Zeppelin was the many benefits that come from leveraging the resource pools.  A resource pool is a block of memory that can be acccessed by all interpreters and is useful for sharing small variables between the interpreters. \\n\\nThe Spark interpreter has a simple interface for accessing the ResourcePools, the Flink interface is less documented but can be reverse engineered from code (thanks open source!)\\n\\n\\nCollect betas from Spark and Flink- compare in Python\\n\\nCreate Matrix in Flink and Spark - visualize with R\",\"title\":\"Use Resource Pools with Zeppelin\"},\"20160928-133502_1743267136\":{\"id\":\"20160928-133502_1743267136\",\"text\":\"%md\\n\\n**NOTE** To install `scatterplot3d` on Ubuntu use:\\n\\n```sh\\nsudo apt-get install r-cran-scatterplot3d\\n```\\n\\n\",\"title\":null},\"20160927-174231_1288588876\":{\"id\":\"20160927-174231_1288588876\",\"text\":\"%sparkMahout\\n\\n\\n\\n\\nz.put(\\\"sparkBeta\\\", beta.asFormatString)\",\"title\":\"Spark ResourcePools\"},\"20160927-181414_1420533932\":{\"id\":\"20160927-181414_1420533932\",\"text\":\"%spark.r {\\\"imageWidth\\\": \\\"400px\\\"}\\n\\nlibrary(\\\"ggplot2\\\")\\n\\nflinkSinStr = z.get(\\\"flinkSinDrm\\\")\\nsparkSinStr = z.get(\\\"sparkSinDrm\\\")\\n\\nflinkData <- read.table(text= flinkSinStr, sep=\\\"\\\\t\\\", header=FALSE)\\nsparkData <- read.table(text= sparkSinStr, sep=\\\"\\\\t\\\", header=FALSE)\\n\\nplot(flinkData,  col=\\\"red\\\")\\n# Graph trucks with red dashed line and square points\\npoints(sparkData, col=\\\"blue\\\")\\n\\n# Create a title with a red, bold\\/italic font\\ntitle(main=\\\"Sampled Mahout Sin Graph in R\\\", col.main=\\\"black\\\", font.main=4)\\n\\nlegend(\\\"bottomright\\\", c(\\\"Apache Flink\\\", \\\"Apache Spark\\\"), col= c(\\\"red\\\", \\\"blue\\\"), pch= c(22, 22)) \\n\\n\",\"title\":null},\"20160927-172629_1189436716\":{\"id\":\"20160927-172629_1189436716\",\"text\":\"%sh\\npython scripts\\/mahout\\/add_mahout_interpreters.py\",\"title\":\"Convenience Paragraph if you started Zeppelin by 'bin\\/zeppelin-daemon.sh start'\"},\"20160928-123353_147363530\":{\"id\":\"20160928-123353_147363530\",\"text\":\"%md\\n\\n## Plotting Mahout with R\\n\\nThe following examples show how we can leverage R to plot our results from Mahout\\n\",\"title\":null},\"20160927-175430_1451783515\":{\"id\":\"20160927-175430_1451783515\",\"text\":\"%spark.pyspark\\n\\nimport ast\\n\\nflinkBetaDict = ast.literal_eval(z.get(\\\"flinkBeta\\\"))\\nsparkBetaDict = ast.literal_eval(z.get(\\\"sparkBeta\\\"))\\n\\nprint \\\"----------------- differences between betas calulated in Flink and Spark-----------------\\\"\\nfor i in range(0,4):\\n    print \\\"beta\\\", i, \\\": \\\" , flinkBetaDict[i] - sparkBetaDict[i]\",\"title\":\"Collect Results in Python and Evaluate Differences\"},\"20160927-163619_899520006\":{\"id\":\"20160927-163619_899520006\",\"text\":\"%flinkMahout\\n\\n\\/\\/ Imports and creating the distributed context, similar but not exactly the same \\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\nimport org.apache.flink.api.scala._\\nimport org.apache.mahout.math.drm._\\nimport org.apache.mahout.math.drm.RLikeDrmOps._\\nimport org.apache.mahout.flinkbindings._\\nimport org.apache.mahout.math._\\nimport scalabindings._\\nimport RLikeOps._\\n\\n\\nimplicit val ctx = new FlinkDistributedContext(benv)\\n\\n\\/\\/ CODE IS EXACTLY THE SAME FROM HERE ON - R-Like DSL \\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\n\\nval drmData = drmParallelize(dense(\\n  (2, 2, 10.5, 10, 29.509541),  \\/\\/ Apple Cinnamon Cheerios\\n  (1, 2, 12,   12, 18.042851),  \\/\\/ Cap'n'Crunch\\n  (1, 1, 12,   13, 22.736446),  \\/\\/ Cocoa Puffs\\n  (2, 1, 11,   13, 32.207582),  \\/\\/ Froot Loops\\n  (1, 2, 12,   11, 21.871292),  \\/\\/ Honey Graham Ohs\\n  (2, 1, 16,   8,  36.187559),  \\/\\/ Wheaties Honey Gold\\n  (6, 2, 17,   1,  50.764999),  \\/\\/ Cheerios\\n  (3, 2, 13,   7,  40.400208),  \\/\\/ Clusters\\n  (3, 3, 13,   4,  45.811716)), numPartitions = 2)\\n  \\ndrmData.collect(::, 0 until 4)\\n\\nval drmX = drmData(::, 0 until 4)\\nval y = drmData.collect(::, 4)\\nval drmXtX = drmX.t %*% drmX\\nval drmXty = drmX.t %*% y\\n\\n\\nval XtX = drmXtX.collect\\nval Xty = drmXty.collect(::, 0)\\nval beta = solve(XtX, Xty)\\n\\n\",\"title\":null},\"20160927-184404_773885252\":{\"id\":\"20160927-184404_773885252\",\"text\":\"%spark.r {\\\"imageWidth\\\": \\\"400px\\\"}\\n\\nlibrary(scatterplot3d)\\n\\n\\nflinkGaussStr = z.get(\\\"flinkGaussDrm\\\")\\nflinkData <- read.table(text= flinkGaussStr, sep=\\\"\\\\t\\\", header=FALSE)\\n\\nscatterplot3d(flinkData, color=\\\"green\\\")\\n\\n\",\"title\":null},\"20160927-162237_1864782562\":{\"id\":\"20160927-162237_1864782562\",\"text\":\"%sh\\n\\npython ..\\/scripts\\/mahout\\/add_mahout.py\",\"title\":\"Convenience Paragraph if you started Zeppelin by '.\\/zeppelin-daemon.sh start'\"},\"20160927-162339_341514150\":{\"id\":\"20160927-162339_341514150\",\"text\":\"%md\\n\\n#### \\\"Installing\\\" the Apache Mahout dependencies and configuring a new Spark and Flink interpreter\\n\\nThe following two paragraphs are convenience paragraphs. You **only need to run them once** to create two new interpreters `%spark.mahout` and `%flink.mahout`. These are intended for users who don't have Apache Mahout already installed. They assume you started Apache Zeppelin from the top level directory or from the bin.  You can tell which one is you by weather you started Zeppelin by typing `.\\/zeppelin-daemon.sh start` or `bin\\/zeppelin-daemon.sh start`.  If you started Zeppelin from somewhere else you will also need to run them from the command line.\\n\\nThey both run a python script which may be found at `ZEPPELIN_HOME\\/scripts\\/mahout\\/add_mahout.py`\\n\\nIn short this script:\\n- Downloads Apache Mahout\\n- Creates a new Flink interpreter with dependencies.\\n- Creates a new Spark interpreter with dependencies and modified configuration to use Kryo serialization.\\n\\n__You only need to run this script once ever.__ (Maybe again if for some reason you delete `conf\\/interpreter.json`) \\n\",\"title\":null},\"20160927-175620_816809523\":{\"id\":\"20160927-175620_816809523\",\"text\":\"%flinkMahout\\nval mxRnd = Matrices.symmetricUniformView(5000, 2, 1234)\\nval drmRand = drmParallelize(mxRnd)\\n\\n\\nval drmSin = drmRand.mapBlock() {case (keys, block) =>  \\n  val blockB = block.like()\\n  for (i <- 0 until block.nrow) {\\n    blockB(i, 0) = block(i, 0) \\n    blockB(i, 1) = Math.sin((block(i, 0) * 8))\\n  }\\n  keys -> blockB\\n}\\n\\nresourcePool.put(\\\"flinkSinDrm\\\", drm.drmSampleToTSV(drmSin, 0.85))\",\"title\":null},\"20160927-174035_1591078106\":{\"id\":\"20160927-174035_1591078106\",\"text\":\"%flinkMahout\\n\\nimport org.apache.zeppelin.interpreter.InterpreterContext\\n\\nval resourcePool = InterpreterContext.get().getResourcePool()\\n\\nresourcePool.put(\\\"flinkBeta\\\", beta.asFormatString)\\n\",\"title\":\"Flink ResourcePools\"},\"20160927-180950_1754833838\":{\"id\":\"20160927-180950_1754833838\",\"text\":\"%sparkMahout\\nval mxRnd = Matrices.symmetricUniformView(5000, 2, 1234)\\nval drmRand = drmParallelize(mxRnd)\\n\\n\\nval drmSin = drmRand.mapBlock() {case (keys, block) =>  \\n  val blockB = block.like()\\n  for (i <- 0 until block.nrow) {\\n    blockB(i, 0) = block(i, 0) \\n    blockB(i, 1) = Math.sin((block(i, 0) * 8))\\n  }\\n  keys -> blockB\\n}\\n\\nz.put(\\\"sparkSinDrm\\\", org.apache.mahout.math.drm.drmSampleToTSV(drmSin, 0.85))\\n\",\"title\":null},\"20160928-135432_2099340527\":{\"id\":\"20160928-135432_2099340527\",\"text\":\"%md\\n\",\"title\":null},\"20160927-165217_1266863511\":{\"id\":\"20160927-165217_1266863511\",\"text\":\"%sparkMahout\\n\\n\\/\\/ Imports and creating the distributed context, similar but not exactly the same \\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\n\\nimport org.apache.mahout.math._\\nimport org.apache.mahout.math.scalabindings._\\nimport org.apache.mahout.math.drm._\\nimport org.apache.mahout.math.scalabindings.RLikeOps._\\nimport org.apache.mahout.math.drm.RLikeDrmOps._\\nimport org.apache.mahout.sparkbindings._\\n\\nimplicit val sdc: org.apache.mahout.sparkbindings.SparkDistributedContext = sc2sdc(sc)\\n\\n\\n\\/\\/ CODE IS EXACTLY THE SAME FROM HERE ON - R-Like DSL \\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\n\\nval drmData = drmParallelize(dense(\\n  (2, 2, 10.5, 10, 29.509541),  \\/\\/ Apple Cinnamon Cheerios\\n  (1, 2, 12,   12, 18.042851),  \\/\\/ Cap'n'Crunch\\n  (1, 1, 12,   13, 22.736446),  \\/\\/ Cocoa Puffs\\n  (2, 1, 11,   13, 32.207582),  \\/\\/ Froot Loops\\n  (1, 2, 12,   11, 21.871292),  \\/\\/ Honey Graham Ohs\\n  (2, 1, 16,   8,  36.187559),  \\/\\/ Wheaties Honey Gold\\n  (6, 2, 17,   1,  50.764999),  \\/\\/ Cheerios\\n  (3, 2, 13,   7,  40.400208),  \\/\\/ Clusters\\n  (3, 3, 13,   4,  45.811716)), numPartitions = 2)\\n  \\ndrmData.collect(::, 0 until 4)\\n\\nval drmX = drmData(::, 0 until 4)\\nval y = drmData.collect(::, 4)\\nval drmXtX = drmX.t %*% drmX\\nval drmXty = drmX.t %*% y\\n\\n\\nval XtX = drmXtX.collect\\nval Xty = drmXty.collect(::, 0)\\nval beta = solve(XtX, Xty)\\n\",\"title\":null},\"20160927-181540_1706054053\":{\"id\":\"20160927-181540_1706054053\",\"text\":\"%flinkMahout\\n\\nval mxRnd3d = Matrices.symmetricUniformView(5000, 3, 1234)\\nval drmRand3d = drmParallelize(mxRnd3d)\\n\\nval drmGauss = drmRand3d.mapBlock() {case (keys, block) =>\\n  val blockB = block.like()\\n  for (i <- 0 until block.nrow) {\\n    val x: Double = block(i, 0)\\n    val y: Double = block(i, 1)\\n    val z: Double = block(i, 2)\\n\\n    blockB(i, 0) = x\\n    blockB(i, 1) = y\\n    blockB(i, 2) = Math.exp(-((Math.pow(x, 2)) + (Math.pow(y, 2)))\\/2)\\n  }\\n  keys -> blockB\\n}\\n\\nresourcePool.put(\\\"flinkGaussDrm\\\", drm.drmSampleToTSV(drmGauss, 50.0))\",\"title\":\"Create a Gaussian Matrix\"},\"20160927-155636_1798325301\":{\"id\":\"20160927-155636_1798325301\",\"text\":\"%md\\n\\n### The [Apache Mahout](http:\\/\\/mahout.apache.org\\/)™ project's goal is to build an environment for quickly creating scalable performant machine learning applications.\\n\\n#### Apache Mahout software provides three major features:\\n\\n- A simple and extensible programming environment and framework for building scalable algorithms\\n- A wide variety of premade algorithms for Scala + Apache Spark, H2O, Apache Flink\\n- Samsara, a vector math experimentation environment with R-like syntax which works at scale\\n\\n#### In other words:\\n\\n*Apache Mahout provides a unified API for quickly creating machine learning algorithms on a variety of engines.*\\n\\n#### Getting Started\\n\\nApache Mahout is a collection of Libraries that enhance Apache Flink, Apache Spark, and others. Currently Zeppelin support the Flink and Spark Engines. A convenience script is provided to setup the nessecary imports and configurations to run Mahout on Spark and Flink. \\n\\nWe can use Apache Mahout's R-Like Domain Specific Language (DSL) inline with native Flink or Spark code.  We must however, first declare a few imports that are different for Spark and Flink\\n\\n__References:__\\n\\n[Mahout-Samsara's In-Core Linear Algebra DSL Reference](http:\\/\\/mahout.apache.org\\/users\\/environment\\/in-core-reference.html)\\n[Mahout-Samsara's Distributed Linear Algebra DSL Reference](http:\\/\\/mahout.apache.org\\/users\\/environment\\/out-of-core-reference.html)\\n[Getting Started with the Mahout-Samsara Shell](http:\\/\\/mahout.apache.org\\/users\\/sparkbindings\\/play-with-shell.html)\\n\",\"title\":null}}"