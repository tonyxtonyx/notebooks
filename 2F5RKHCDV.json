{"paragraph_1586733774605_1418179269":{"orderNumber":0,"id":"paragraph_1586733774605_1418179269","text":"%flink \n\nimport org.apache.flink.streaming.api.functions.source.SourceFunction\nimport org.apache.flink.table.api.TableEnvironment\nimport org.apache.flink.streaming.api.TimeCharacteristic\nimport org.apache.flink.streaming.api.checkpoint.ListCheckpointed\nimport java.util.Collections\nimport scala.collection.JavaConversions._\n\nsenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\nsenv.enableCheckpointing(1000)\n\nval data = senv.addSource(new SourceFunction[(Long, String)] with ListCheckpointed[java.lang.Long] {\n\n  val pages = Seq(\"home\", \"search\", \"search\", \"product\", \"product\", \"product\")\n  var count: Long = 0\n  var running : Boolean = true\n  // startTime is 2020/1/1\n  var startTime: Long = new java.util.Date(2020 - 1900,0,1).getTime\n  var sleepInterval = 100\n\n  override def run(ctx: SourceFunction.SourceContext[(Long, String)]): Unit = {\n    val lock = ctx.getCheckpointLock\n\n    while (count < 3000 && running) {\n      lock.synchronized({\n        ctx.collect((startTime + count * sleepInterval, pages(count.toInt % pages.size)))\n        count += 1\n        Thread.sleep(sleepInterval)\n      })\n    }\n  }\n\n  override def cancel(): Unit = {\n    running = false\n  }\n\n  override def snapshotState(checkpointId: Long, timestamp: Long): java.util.List[java.lang.Long] = {\n    Collections.singletonList(count)\n  }\n\n  override def restoreState(state: java.util.List[java.lang.Long]): Unit = {\n    state.foreach(s => count = s)\n  }\n\n}).assignAscendingTimestamps(_._1)\n\nstenv.registerDataStream(\"log\", data, 'time, 'url, 'rowtime.rowtime)\n","title":"Register Data Source"},"paragraph_1586847370895_154139610":{"orderNumber":1,"id":"paragraph_1586847370895_154139610","text":"%flink.ssql(type=update)\n\nselect url, count(1) as c from log group by url","title":"Resume flink sql job without savepoint"}}