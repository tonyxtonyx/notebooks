{"paragraph_1586733774605_1418179269":{"orderNumber":1,"id":"paragraph_1586733774605_1418179269","text":"%flink \n\nimport org.apache.flink.streaming.api.functions.source.SourceFunction\nimport org.apache.flink.table.api.TableEnvironment\nimport org.apache.flink.streaming.api.TimeCharacteristic\nimport org.apache.flink.streaming.api.checkpoint.ListCheckpointed\nimport java.util.Collections\nimport scala.collection.JavaConversions._\n\nsenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\nsenv.enableCheckpointing(1000)\n\nval data = senv.addSource(new SourceFunction[(Long, String)] with ListCheckpointed[java.lang.Long] {\n\n  val pages = Seq(\"home\", \"search\", \"search\", \"product\", \"product\", \"product\")\n  var count: Long = 0\n  var running : Boolean = true\n  // startTime is 2020/1/1\n  var startTime: Long = new java.util.Date(2020 - 1900,0,1).getTime\n  var sleepInterval = 100\n\n  override def run(ctx: SourceFunction.SourceContext[(Long, String)]): Unit = {\n    val lock = ctx.getCheckpointLock\n\n    while (count < 3000 && running) {\n      lock.synchronized({\n        ctx.collect((startTime + count * sleepInterval, pages(count.toInt % pages.size)))\n        count += 1\n        Thread.sleep(sleepInterval)\n      })\n    }\n  }\n\n  override def cancel(): Unit = {\n    running = false\n  }\n\n  override def snapshotState(checkpointId: Long, timestamp: Long): java.util.List[java.lang.Long] = {\n    Collections.singletonList(count)\n  }\n\n  override def restoreState(state: java.util.List[java.lang.Long]): Unit = {\n    state.foreach(s => count = s)\n  }\n\n}).assignAscendingTimestamps(_._1)\n\nstenv.registerDataStream(\"log\", data, 'time, 'url, 'rowtime.rowtime)\n","title":"Register Data Source"},"paragraph_1586754954622_-1794803125":{"orderNumber":5,"id":"paragraph_1586754954622_-1794803125","text":"%flink.ipyflink(parallelism=1,maxParallelism=10,savepointDir=/tmp/flink_c)\n\ntable = st_env.sql_query(\"select url, count(1) as pv from log group by url\")\n\nz.show(table, stream_type=\"update\")","title":"Resume flink python job from savepoint"},"paragraph_1587964310955_443124874":{"orderNumber":0,"id":"paragraph_1587964310955_443124874","text":"%md\n\n# Introduction\n\nThis tutorial is to demonstrate how to do job control in flink (job submission/cancel/resume).\n2 steps:\n1. Create custom data stream and register it as flink table. The custom data stream is a simulated web access logs. \n2. Query this flink table (pv for each page type), you can cancel it and then resume it again w/o savepoint.\n","title":"Introduction"},"paragraph_1586847370895_154139610":{"orderNumber":2,"id":"paragraph_1586847370895_154139610","text":"%flink.ssql(type=update)\n\nselect url, count(1) as c from log group by url","title":"Resume flink sql job without savepoint"},"paragraph_1586733868269_783581378":{"orderNumber":4,"id":"paragraph_1586733868269_783581378","text":"%flink(parallelism=1,maxParallelism=10,savepointDir=/tmp/flink_b)\n\nval table = stenv.sqlQuery(\"select url, count(1) as pv from log group by url\")\n\nz.show(table, streamType=\"update\")\n","title":"Resume flink scala job from savepoint"},"paragraph_1587207857968_1997116221":{"orderNumber":6,"id":"paragraph_1587207857968_1997116221","text":"%flink.ssql\n","title":null},"paragraph_1586733780533_1100270999":{"orderNumber":3,"id":"paragraph_1586733780533_1100270999","text":"%flink.ssql(type=update,parallelism=2,maxParallelism=10,savepointDir=/tmp/flink_a)\n\nselect url, count(1) as pv from log group by url","title":"Resume flink sql job from savepoint"}}