{"paragraph_1579054784565_2122156822":{"orderNumber":0,"id":"paragraph_1579054784565_2122156822","text":"%md\n\nThis tutorial demonstrate how to use Flink do streaming analytics via its streaming sql + udf. Zeppelin now support 3 kinds of streaming visualization.\n\n* Single  - Single mode is for the case when the result of sql statement is always one row.\n* Update  - Update mode is suitable for the case when the output is more than one rows, and always will be updated continuously. \n* Append  - Append mode is suitable for the scenario where output data is always appended\n\n","title":"Overview"},"paragraph_1578910016872_1942851900":{"orderNumber":4,"id":"paragraph_1578910016872_1942851900","text":"%flink.ssql(type=append,parallelism=1,refreshInterval=2000,threshold=60000)\n\nselect TUMBLE_START(rowtime,INTERVAL '5' SECOND) start_time,url,count(1) as pv from log\ngroup by TUMBLE(rowtime,INTERVAL '5' SECOND),url\n","title":"Append mode of Output"},"paragraph_1578921455738_-1465781668":{"orderNumber":5,"id":"paragraph_1578921455738_-1465781668","text":"%flink.ssql\n","title":null},"paragraph_1578910004762_-286113604":{"orderNumber":3,"id":"paragraph_1578910004762_-286113604","text":"%flink.ssql(type=update,parallelism=1,refreshInterval=1000)\n\nselect url,count(1) as pv from log group by url\n","title":"Update mode of Output"},"paragraph_1611556011274_1848600588":{"orderNumber":1,"id":"paragraph_1611556011274_1848600588","text":"%flink \n\nimport org.apache.flink.streaming.api.functions.source.SourceFunction\nimport org.apache.flink.table.api.TableEnvironment\nimport org.apache.flink.streaming.api.TimeCharacteristic\nimport org.apache.flink.streaming.api.checkpoint.ListCheckpointed\nimport java.util.Collections\nimport scala.collection.JavaConversions._\n\nsenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\nsenv.enableCheckpointing(1000)\n\nval data = senv.addSource(new SourceFunction[(Long, String)] with ListCheckpointed[java.lang.Long] {\n\n  val pages = Seq(\"home\", \"search\", \"search\", \"product\", \"product\", \"product\")\n  var count: Long = 0\n  var running : Boolean = true\n  // startTime is 2020/1/1\n  var startTime: Long = new java.util.Date(2020 - 1900,0,1).getTime\n  var sleepInterval = 100\n\n  override def run(ctx: SourceFunction.SourceContext[(Long, String)]): Unit = {\n    val lock = ctx.getCheckpointLock\n\n    while (count < 300000 && running) {\n      lock.synchronized({\n        ctx.collect((startTime + count * sleepInterval, pages(count.toInt % pages.size)))\n        count += 1\n        Thread.sleep(sleepInterval)\n      })\n    }\n  }\n\n  override def cancel(): Unit = {\n    running = false\n  }\n\n  override def snapshotState(checkpointId: Long, timestamp: Long): java.util.List[java.lang.Long] = {\n    Collections.singletonList(count)\n  }\n\n  override def restoreState(state: java.util.List[java.lang.Long]): Unit = {\n    state.foreach(s => count = s)\n  }\n\n}).assignAscendingTimestamps(_._1)\n\nstenv.registerDataStream(\"log\", data, 'time, 'url, 'rowtime.rowtime)\n","title":null},"paragraph_1578909960516_-1812187661":{"orderNumber":2,"id":"paragraph_1578909960516_-1812187661","text":"%flink.ssql(type=single,parallelism=1,refreshInterval=1000,template=<h1>{1}</h1> until <h2>{0}</h2>)\n\nselect max(rowtime),count(1) from log\n","title":"Single row mode of Output"}}